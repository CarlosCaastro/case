{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c098e5f1-d791-44f9-ac49-b574985fd17b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, Row\n",
    "\n",
    "class DataExtractorIfood:\n",
    "    def __init__(self, spark: SparkSession, user: str, token: str):\n",
    "        self.spark = spark\n",
    "        self.user = user\n",
    "        self.headers = {\"Authorization\": f\"token {token}\"}\n",
    "\n",
    "    def get_followers(self):\n",
    "        followers = []\n",
    "        page = 1\n",
    "        while True:\n",
    "            end_point_followers = f'https://api.github.com/users/{self.user}/followers?page={page}'\n",
    "            response_followers = requests.get(end_point_followers, headers=self.headers).json()\n",
    "            if not response_followers:\n",
    "                break\n",
    "            followers.extend(response_followers)\n",
    "            page += 1\n",
    "        users = [{'login': follower['login']} for follower in followers]\n",
    "        followers_df = self.spark.createDataFrame(users)\n",
    "        return followers_df\n",
    "\n",
    "    def get_github_user_info(self, login):\n",
    "        url = f\"https://api.github.com/users/{login}\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        if response.status_code == 200:\n",
    "            user_info = response.json()\n",
    "            return (\n",
    "                user_info.get('name'),\n",
    "                user_info.get('company'),\n",
    "                user_info.get('blog'),\n",
    "                user_info.get('email'),\n",
    "                user_info.get('bio'),\n",
    "                user_info.get('public_repos'),\n",
    "                user_info.get('followers'),\n",
    "                user_info.get('following'),\n",
    "                user_info.get('created_at')\n",
    "            )\n",
    "        else:\n",
    "            return (None, None, None, None, None, None, None, None, None)\n",
    "\n",
    "    def enrich_with_github_info(self, df):\n",
    "        users = df.collect()\n",
    "\n",
    "        enriched_data = []\n",
    "        for user in users:\n",
    "            user_info = self.get_github_user_info(user['login'])\n",
    "            enriched_data.append(Row(\n",
    "                name=user_info[0],\n",
    "                company=user_info[1],\n",
    "                blog=user_info[2],\n",
    "                email=user_info[3],\n",
    "                bio=user_info[4],\n",
    "                public_repos=user_info[5],\n",
    "                followers=user_info[6],\n",
    "                following=user_info[7],\n",
    "                created_at=user_info[8]\n",
    "            ))\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"company\", StringType(), True),\n",
    "            StructField(\"blog\", StringType(), True),\n",
    "            StructField(\"email\", StringType(), True),\n",
    "            StructField(\"bio\", StringType(), True),\n",
    "            StructField(\"public_repos\", IntegerType(), True),\n",
    "            StructField(\"followers\", IntegerType(), True),\n",
    "            StructField(\"following\", IntegerType(), True),\n",
    "            StructField(\"created_at\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        enriched_df = self.spark.createDataFrame(enriched_data, schema)\n",
    "        return enriched_df\n",
    "\n",
    "    def execute_extract_api(self):\n",
    "        df = self.get_followers()\n",
    "        df_extract = self.enrich_with_github_info(df=df)\n",
    "\n",
    "        return df_extract\n",
    "\n",
    "    def read_csv(self, path):\n",
    "        return self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    def get_count_followers(self):\n",
    "        url = f\"https://api.github.com/users/{self.user}\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        data = response.json()\n",
    "        followers_count = data.get('followers')\n",
    "\n",
    "        return followers_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f247347d-98a8-4979-a332-700d1f050a4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import regexp_replace, to_date, date_format\n",
    "\n",
    "class DataLoaderIfood:\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "\n",
    "    def save_to_csv(self, df: DataFrame):\n",
    "        df.coalesce(1).write.csv(self.path, header=True, mode=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "91b7a698-b4ec-4950-93b5-00cd147950cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import regexp_replace, to_date, date_format\n",
    "\n",
    "class DataTransformerIfood:\n",
    "    @staticmethod\n",
    "    def transform(df: DataFrame) -> DataFrame:\n",
    "        df_transformed = df.withColumn(\"company\", regexp_replace(\"company\", \"@\", \"\")) \\\n",
    "                           .withColumn(\"created_at\", date_format(to_date(\"created_at\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"), \"dd/MM/yyyy\"))\n",
    "        return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1c7a9455-5a87-422f-9778-2d07ee6f6582",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  col, regexp_extract\n",
    "\n",
    "class DataTestIfood:\n",
    "\n",
    "    def __init__(self, spark: SparkSession,path,user, token):\n",
    "        self.spark = spark\n",
    "        self.path = path\n",
    "        self.user = user\n",
    "        self.token = token\n",
    "        self.headers = {\"Authorization\": f\"token {token}\"}\n",
    "\n",
    "    def read_data(self):\n",
    "        df_test = self.spark.read.format(\"csv\").options(header = True, multiLine=True).load(self.path)\n",
    "        \n",
    "        return df_test\n",
    "\n",
    "    def clean_company(self,df):\n",
    "        \n",
    "        if df.filter(col(\"company\").like(\"%@%\")).count() > 0:\n",
    "            print(\"Existe pelo menos um '@' na coluna 'company'.\")\n",
    "        else:\n",
    "            print(\"Não existe '@' na coluna 'company'.\")\n",
    "        \n",
    "    def date_format(self,df):\n",
    "\n",
    "        pattern = r'^\\d{2}/\\d{2}/\\d{4}$'\n",
    "\n",
    "        if df.filter(regexp_extract(col(\"created_at\"), pattern, 0) != \"\").count() == df.count():\n",
    "            print(\"A coluna 'created_at' está com as datas no formato dd/mm/yyyy.\")\n",
    "        else:\n",
    "            print(\"A coluna 'created_at' não está com todas as datas no formato dd/mm/yyyy.\")\n",
    "    \n",
    "    def count_followers(self,df):\n",
    "\n",
    "        followers_count = DataExtractorIfood(spark=self.spark,user=self.user,token=self.token).get_count_followers()\n",
    "        \n",
    "        if df.count() == followers_count:\n",
    "            return print(\"A quantidade de seguidores em conformidade a quantidade atual de seguidores no GitHub.\")\n",
    "        else:\n",
    "            return print(\"A quantidade de seguidores não está igual\")\n",
    "\n",
    "\n",
    "    def execute_test(self):\n",
    "        print(\"Resultado das verificações de teste no Dataframe.\")\n",
    "        df = self.read_data()\n",
    "        self.clean_company(df=df)\n",
    "        self.date_format(df=df)\n",
    "        self.count_followers(df=df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc4120a-842b-4665-a693-43e50431f391",
     "showTitle": true,
     "title": "Constantes"
    }
   },
   "outputs": [],
   "source": [
    "user = 'marciocl'\n",
    "token = ''\n",
    "path = '/app/output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e439df-7f85-4a5b-8f2a-232a4ac10875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\n",
    "spark.conf.set(\"parquet.enable.summary-metadata\", \"false\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024d84bf-322a-4e94-8aa5-9135176cdb3a",
     "showTitle": true,
     "title": "Instanciação dos Objetos"
    }
   },
   "outputs": [],
   "source": [
    "extractor = DataExtractorIfood(spark=spark, user=user, token=token)\n",
    "transformer = DataTransformerIfood()\n",
    "loader = DataLoaderIfood(path)\n",
    "test = DataTestIfood(spark=spark,path=path,user=user,token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fe1c02-57ad-4057-a345-2487e392bdc1",
     "showTitle": true,
     "title": "Extração"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado das verificações de teste no Dataframe.\n",
      "Não existe '@' na coluna 'company'.\n",
      "A coluna 'created_at' está com as datas no formato dd/mm/yyyy.\n",
      "A quantidade de seguidores em conformidade a quantidade de seguidores no GitHub.\n"
     ]
    }
   ],
   "source": [
    "df_with_github_info = extractor.execute_extract_api()\n",
    "\n",
    "df_transformed = transformer.transform(df_with_github_info)\n",
    "\n",
    "loader.save_to_csv(df_transformed)\n",
    "\n",
    "test.execute_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9425d92b-8e6f-4044-9c45-15154e0e4a06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87b7225-844f-47fe-9a3a-9cadcf891372",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  col, regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b03def7-d0a0-4677-9a89-60de486805ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test = spark.read.format(\"csv\").options(header = True, multiLine=True).load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe65eb9-f440-4481-9d85-fc7d417d6369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if df_test.filter(col(\"company\").like(\"%@%\")).count() > 0:\n",
    "    print(\"Existe pelo menos um '@' na coluna 'company'.\")\n",
    "else:\n",
    "    print(\"Não existe '@' na coluna 'company'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9240a3-0a3b-40f6-bb6b-b5bf21bc3527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pattern = r'^\\d{2}/\\d{2}/\\d{4}$'\n",
    "if df_test.filter(regexp_extract(col(\"created_at\"), pattern, 0) != \"\").count() == df_test.count():\n",
    "    print(\"A coluna 'created_at' está com as datas no formato dd/mm/yyyy.\")\n",
    "else:\n",
    "    print(\"A coluna 'created_at' não está com todas as datas no formato dd/mm/yyyy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ae372b-905b-47a6-a674-9077efa3222b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/app/output/part-00000-2e540982-f004-4f07-8dd0-e4121f062088-c000.csv\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ifood - People Analytics - Modularização",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
